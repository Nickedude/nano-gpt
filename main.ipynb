{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of dataset: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f08d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print(\"Unique characters: \", \"\".join(chars))\n",
    "print(f\"Size: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to tokenize!\n",
    "# This means converting the string, i.e a sequence of characters, to a sequence of integers according to some vocabulary of possible elements.\n",
    "# In our case, each character is a token.\n",
    "\n",
    "# Create mapping from characters to integers\n",
    "stoi = {c: i for i,c in enumerate(chars)}\n",
    "itos = {i: c for i,c in enumerate(chars)}\n",
    "\n",
    "# Encoder and decoder\n",
    "encode = lambda s: [stoi[c] for c in s]  # Encodes string (list of characters) to list of integers\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])  # Decodes a list of integers to a string\n",
    "\n",
    "print(encode(\"Hi there\"))\n",
    "print(decode(encode(\"Hi there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire text dataset and store it into a tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])  # The 1000 characters we looked at earlier will be presented like this to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4945a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split into train/eval\n",
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "eval_data = data[train_size:]\n",
    "\n",
    "print(train_data.shape, eval_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2329ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the context length, also called block size\n",
    "# This is the amount of text the transformer will see during training\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize the samples the transformer will see\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"When input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8295c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce batch dimension\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split: str, batch_size: int):\n",
    "    # Generate a batch of inputs and targets\n",
    "    data = train_data if split == \"train\" else eval_data\n",
    "    indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in indices])\n",
    "    return x,y\n",
    "    \n",
    "xs, ys = get_batch(\"train\", batch_size)\n",
    "print(\"Inputs: \")\n",
    "print(xs.shape)\n",
    "print(xs)\n",
    "print(\"Targets: \")\n",
    "print(ys.shape)\n",
    "print(ys)\n",
    "print(\"------\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for i in range(block_size):\n",
    "        context = xs[b,:i+1]\n",
    "        target = ys[b,i]\n",
    "        print(f\"When input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs)  # Our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"This model predicts the next character based on a single character.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads the logits for the next toke from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None) -> torch.Tensor:\n",
    "        # idx and targets are [batch_size, block_size] tensors of integers\n",
    "        logits = self.token_embedding_table(idx)  # [batch_size, block_size, vocabulary_size]\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape to [batch_size * block_size, vocabulary_size] according to what cross_entropy wants\n",
    "            batch_size, block_size, vocabulary_size = logits.shape\n",
    "            logits = logits.view(batch_size*block_size, vocabulary_size)\n",
    "            targets = targets.view(batch_size*block_size)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a [batch_size, block_size] tensor of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)  # Get predictions\n",
    "            logits = logits[:, -1, :]  # Focus on last element, i.e what comes next\n",
    "            probabilities = F.softmax(logits, dim=-1)  # Softmax logits to get probabilities\n",
    "            next_idx = torch.multinomial(probabilities, num_samples=1)  # [batch_size, 1]\n",
    "            idx = torch.cat((idx, next_idx), dim=1)  # [batch_size, block_size + 1]\n",
    "            \n",
    "        return idx\n",
    "            \n",
    "            \n",
    "    \n",
    "model = BigramLanguageModel(vocabulary_size)\n",
    "out, loss = model(xs, ys)\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc64def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10_000):\n",
    "    xs, ys = get_batch(\"train\", batch_size)\n",
    "    \n",
    "    logits, loss = model(xs, ys)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d516538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(model.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ead592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
